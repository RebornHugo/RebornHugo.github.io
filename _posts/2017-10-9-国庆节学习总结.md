国庆的前四天窝在宿舍学习了四天的deep learning，主要关于unsupervised learning 和 generative model 在内的各种算法和模型。

学习材料：

1. ntu李弘毅教授的slide和video
2. cs231n
3. Ian Goodfellow 大佬的《deep learning》
4. 包括但不限于Andrej在内的各种blog和知乎专栏等
5. 各种Paper

# PCA

principle component analysis，由于线代没学好，我对PCA的具体推导过程仍存有疑惑。其中主要用到的SVD奇异值分解更是100本所未曾提过的知识点，需要重新学过。PCA可以看作是一种最简单的autoencoder，但他的运算效率显然比使用gradient descent的autoender高得多。

## core model

$Z_1 = W^1\cdot x$                  $X$是原始数据，通过与$W$相乘将其投影到低围空间。PCA要做的事情是找到一个对应的$W$使得得到的$Z$的variance最大。

## formula

![PCA image1](/assets/images/post_images/国庆学习总结/PCA1.png)

![PCA image2](/assets/images/post_images/国庆学习总结/PCA2.png)

## weakness of PCA

1. 相比于deep autoencoder，不够deep
2. 只能做线性投影，但可通过含kernel的autoencoder进行改善

# Neighbor Embedding

## LLE

Locally Liner Embedding 

## Laplacian Eigenmaps

## t-SNE

T-distributed Stochastic Neighbor Embedding

> normally, use it after PCA

# autoencoder

## deep autoencoder

> minimize reconstructive model

## variational autoencoder(VAE)

# word embedding

> using word embedding to replace **one-hot encoding** to represent word or sentence. 

## GloVe

## Prediction-Based 

# RNN

recurrent neural network

## lstm

# Gans
